services:
  # api:
  #   build: .
  #   env_file: .env
  #   depends_on:
  #     - postgres
  #     - redis
  #     - opensearch
  #     - qdrant
  #     - neo4j
  #     - ollama
  #   ports:
  #     - "8080:8080"
  #   volumes:
  #     - uploads:/data/uploads

  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: proto
    volumes:
      - "pgdata:/var/lib/postgresql"
      - "./db/init:/docker-entrypoint-initdb.d"
    ports:
      - "5432:5432"

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
      PGADMIN_CONFIG_SERVER_MODE: "False"
    ports:
      - "5050:80"
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      - postgres

  opensearch:
    image: opensearchproject/opensearch:latest
    environment:
      OPENSEARCH_INITIAL_ADMIN_PASSWORD: ${OPENSEARCH_ADMIN_PASSWORD}
      discovery.type: single-node
      plugins.security.disabled: "true"
      OPENSEARCH_JAVA_OPTS: "-Xms1g -Xmx1g"
    ports:
      - "9200:9200"
      - "9600:9600"
    volumes:
      - "osdata:/usr/share/opensearch/data"

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:latest
    container_name: opensearch-dashboards
    ports:
      - "5601:5601"
    environment:
      OPENSEARCH_HOSTS: '["http://opensearch:9200"]'
      DISABLE_SECURITY_DASHBOARDS_PLUGIN: "true"
    depends_on:
      - opensearch

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - "qdrant:/qdrant/storage"

  neo4j:
    image: neo4j:5-community
    environment:
      NEO4J_AUTH: ${NEO4J_USER}/${NEO4J_PASSWORD}
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_ACCEPT_LICENSE_AGREEMENT: yes
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*"
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - "neo4j:/data"
      
  redis:
    image: redis:latest
    ports:
      - "6379:6379"

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - "ollama:/root/.ollama"
    environment:
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]

  # # vLLM - High-performance LLM inference with OpenAI-compatible API
  # # Vorteile: Continuous Batching, PagedAttention, bessere Skalierung
  # vllm:
  #   image: vllm/vllm-openai:latest
  #   container_name: vllm
  #   ports:
  #     - "8001:8000"
  #   volumes:
  #     - "vllm_models:/root/.cache/huggingface"
  #   environment:
  #     - HF_TOKEN=${HF_TOKEN:-}
  #     - VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
  #   command: >
  #     --model Qwen/Qwen2.5-7B-Instruct-AWQ
  #     --quantization awq_marlin
  #     --host 0.0.0.0
  #     --port 8000
  #     --max-model-len 8192
  #     --gpu-memory-utilization 0.60
  #     --enable-prefix-caching
  #     --dtype auto
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - capabilities: ["gpu"]
  #             count: 1
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 180s
  #   profiles:
  #     - vllm  # Nur starten mit: docker compose --profile vllm up

volumes:
  pgdata: {}
  osdata: {}
  qdrant: {}
  neo4j: {}
  uploads: {}
  ollama: {}
  pgadmin_data: {}
 # vllm_models: {}
