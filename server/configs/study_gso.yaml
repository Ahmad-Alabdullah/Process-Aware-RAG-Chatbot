baseline: "configs/baseline.yaml"

description: |
  Winner Combination: Greedy Sequential Optimization
  
  Ablauf:
  1. Baseline → OFAT Runs
  2. Beste OFAT-Änderungen identifizieren (Δ > 0.02 in primary metric)
  3. Sequentiell kombinieren und testen
  
  Wissenschaftliche Basis:
  - Bergstra & Bengio (2012): Random Search for Hyper-Parameter Optimization
  - Probst et al. (2019): Tunability: Importance of Hyperparameters

# Nach OFAT: Die Gewinner manuell hier eintragen
# Beispiel nach OFAT-Analyse:
winners_from_ofat:
  - factor: "rerank"
    delta: "+0.08 recall@5"
    config: {factors: {rerank: {enabled: true, model: "jina-reranker-v3", top_n: 50}}}
  
  - factor: "rrf_k"
    delta: "+0.03 recall@5"
    config: {factors: {retrieval: {rrf_k: 90}}}
  
  - factor: "top_k"
    delta: "+0.02 recall@5"
    config: {qa_payload: {top_k: 8}}

# Sequentielle Kombinationen
variants:
  # Step 1: Beste Änderung
  - name: "WIN_step1_rerank"
    override:
      factors:
        rerank:
          enabled: true
          model: "jina-reranker-v3"
          top_n: 50
    description: "Beste Einzeländerung: Reranking aktivieren"

  # Step 2: + Zweitbeste
  - name: "WIN_step2_rerank_rrf"
    override:
      factors:
        rerank:
          enabled: true
          model: "jina-reranker-v3"
          top_n: 50
        retrieval:
          rrf_k: 90
    description: "Reranking + RRF_K=90"

  # Step 3: + Drittbeste
  - name: "WIN_step3_rerank_rrf_k8"
    override:
      qa_payload:
        top_k: 8
      factors:
        rerank:
          enabled: true
          model: "jina-reranker-v3"
          top_n: 50
        retrieval:
          rrf_k: 90
    description: "Reranking + RRF_K=90 + Top-K=8"

# Stopp-Kriterium: Wenn Δ < 0.01 gegenüber vorherigem Step
stop_criterion:
  min_delta: 0.01
  primary_metric: "recall@5"